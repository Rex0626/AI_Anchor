import os
import json
import re
from datetime import timedelta
from moviepy.editor import VideoFileClip
from vertexai.generative_models import Part
from haystack_integrations.components.generators.google_vertex import VertexAIGeminiGenerator
from haystack import component, Pipeline
from haystack.components.builders import PromptBuilder
from tqdm import tqdm

# ========== 1. è¨­å®šèˆ‡æ†‘è­‰ ==========
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
cred_path = os.path.join(PROJECT_ROOT, "credentials", "ai-anchor-462506-7887b7105f6a.json")
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = cred_path

# ========== 2. é—œéµåƒæ•¸ ==========
SYLLABLES_PER_SEC = 4.8       
MIN_EVENT_DURATION = 1.0      
MAX_RALLY_DURATION = 6.0
MIN_GAP_DURATION = 3.0        
MAX_INTRO_OUTRO_SYLLABLES = 30 
MERGE_THRESHOLD = 1.2 # [æ–°å¢] å¼·åˆ¶åˆä½µé–¾å€¼ï¼šè‹¥ç‰‡æ®µçŸ­æ–¼ 1.2 ç§’ï¼Œå¼·åˆ¶åˆä½µåˆ°ä¸‹ä¸€æ®µ

# å…¨åŸŸæ­·å²ç´€éŒ„
NARRATIVE_HISTORY = [] 
HISTORY_WINDOW_SIZE = 3 

# ========== 3. å·¥å…·å‡½æ•¸ ==========
def seconds_to_timecode(seconds):
    m, s = divmod(seconds, 60)
    h, m = divmod(m, 60)
    return f"{int(h)}:{int(m):02d}:{s:04.1f}"

def format_duration(seconds):
    m = int(seconds // 60)
    s = seconds % 60
    return f"{m}:{s:04.1f}"

def parse_time_str(t_str):
    try:
        if not t_str: return 0.0
        parts = t_str.strip().split(':')
        sec = 0.0
        if len(parts) == 3: sec += float(parts[-3]) * 3600
        if len(parts) >= 2: sec += float(parts[-2]) * 60
        sec += float(parts[-1])
        return sec
    except: return 0.0

def estimate_speech_time(text):
    if not text: return 0.0
    chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
    text_no_zh = re.sub(r'[\u4e00-\u9fff]', ' ', text)
    text_clean_en = re.sub(r'[^\w\s]', '', text_no_zh)
    english_words = text_clean_en.split()
    count_punc = len(re.findall(r'[ï¼Œã€‚ï¼,.]', text))
    total_units = (len(chinese_chars) * 1.0) + (len(english_words) * 1.3) + (count_punc * 0.4)
    return total_units / SYLLABLES_PER_SEC

# ========== 4. Pipeline åˆå§‹åŒ– ==========
@component
class AddVideo2Prompt:
    @component.output_types(prompt=list)
    def run(self, uri: str, prompt: str):
        return {"prompt": [Part.from_uri(uri, mime_type="video/mp4"), prompt]}

@component
class GeminiGenerator:
    def __init__(self, project_id, location, model):
        self.project_id, self.location, self.model = project_id, location, model
    @component.output_types(replies=list)
    def run(self, prompt: list):
        generator = VertexAIGeminiGenerator(project_id=self.project_id, location=self.location, model=self.model)
        return {"replies": generator.run(prompt)["replies"]}

# ========== 5. Prompt æ¨¡æ¿ ==========
narrative_template = """ 
1. è§’è‰² (Role)
ä½ æ˜¯ä¸€ä½**è³‡æ·±ã€ç†±è¡€ä¸”ç¯€å¥æ˜å¿«**çš„ç¾½çƒè³½äº‹å³æ™‚ä¸»æ’­ã€‚
ä½ çš„è²éŸ³å……æ»¿æ¿€æƒ…ï¼Œèƒ½ç²¾æº–æ•æ‰è³½å ´ä¸Šçš„æ¯ä¸€å€‹ç²¾å½©ç¬é–“ã€‚

2. å‰æƒ…æè¦ (Context)
- **æ­·å²æˆ°æ³å›é¡§**ï¼š
{{ prev_context }}
*(è«‹åƒè€ƒä¸Šè¿°æ­·å²ç´€éŒ„ï¼ŒæŒæ¡æ¯”è³½æ°£å‹¢æµå‘)*

- **é›™æ¨¡æ…‹è³‡è¨Š**ï¼šè«‹çµåˆ **JSON æ•¸æ“š (éª¨æ¶)** èˆ‡ **å½±ç‰‡ç•«é¢ (è¡€è‚‰)** é€²è¡Œè§£èªªã€‚

3. æ‡‰è©²è¦åšçš„äº‹ (Tasks)
- **å€åˆ†å ´æ™¯èˆ‡ç¯€å¥ (Pacing)**ï¼š
    - **ğŸŸ¢ INTRO**: æš–å ´ï¼Œå¸¶å…¥æ°£æ°›ã€‚
    - **ğŸŸ¡ RALLY (æ¿€å‹•)**: èªé€Ÿå¿«ï¼ç·Šè·Ÿçƒè·¯ã€‚è‹¥æœ‰é€£çºŒæ”»é˜²ï¼Œè«‹ç”¨æµæš¢èªå¥ä¸²è¯ã€‚
    - **ğŸ”µ GAP (èˆ’ç·©)**: ç•¶å…§å®¹æ¨™è¨»ç‚ºã€Œä¸­å ´é–“éš™ã€æ™‚ï¼Œè«‹æ”¾æ…¢èªé€Ÿã€‚å¡«è£œå…§å®¹åƒ…é™ï¼š**è©•è«–ä¸Šä¸€çƒå¾—å¤±ã€æè¿°çƒå“¡ç‹€æ…‹ã€æˆ–åˆ†æå¿ƒç†**ã€‚
    - **ğŸ”´ OUTRO**: ç¸½çµæœ¬æ®µè½çµæœã€‚
- **äººåé‡è¿°**ï¼šå‹™å¿…å¸¶ä¸Šçƒå“¡åå­—ï¼Œç‰¹åˆ¥æ˜¯åœ¨æ”»é˜²è½‰æ›æ™‚ã€‚
- **è¦–è¦ºç´°ç¯€**ï¼šæè¿°æ®ºçƒçš„ã€Œè²éŸ³ã€ã€æ•‘çƒçš„ã€Œç‹¼ç‹½ã€ã€æ…¶ç¥çš„ã€Œå‹•ä½œã€ã€‚

4. ç¦æ­¢åšçš„äº‹ (Strict Prohibitions)
â›”ï¸ **åš´æ ¼ç¦ä»¤ (é•è€…å°è‡´æ’­å ±äº‹æ•…)ï¼š**
- **ğŸˆ² é–“éš™å¹»è¦º (No Action in Gap)**ï¼šåœ¨ `GAP` æ™‚æ®µï¼Œ**çµ•å°ç¦æ­¢**æè¿°ä»»ä½•æ“Šçƒå‹•ä½œï¼ˆå¦‚ç™¼çƒã€æ®ºçƒï¼‰ã€‚é€™æ˜¯æ­»çƒæ™‚é–“ï¼Œåªèƒ½è¬›éœæ…‹å…§å®¹ã€‚
- **ğŸˆ² ç¦æ­¢è…¦è£œçµæœ**ï¼šè‹¥è¼¸å…¥å…§å®¹æåˆ°ã€Œç•«é¢ä¸­æ–·ã€æˆ–ã€Œçƒæœªè½åœ°ã€ï¼Œ**çµ•å°ä¸å¯**å®£å‘Šå¾—åˆ†æˆ–ç•Œå¤–ã€‚
- **ğŸˆ² ç¦æ­¢éºæ¼ (No Skipping)**ï¼šè¼¸å…¥åˆ—è¡¨ä¸­çš„æ¯ä¸€å€‹ ID éƒ½å¿…é ˆå°æ‡‰ä¸€å¥è§£èªªï¼Œä¸å¯è·³éä»»ä½•ä¸€å€‹å‹•ä½œå€å¡Šã€‚
- **ğŸˆ² çµ•å°ä¸å¯è¶…æ™‚**ï¼šåš´æ ¼éµå®ˆ `constraint` éŸ³ç¯€é™åˆ¶ã€‚

5. JSON æ¬„ä½å®šç¾©
è¼¸å‡ºç´” JSON é™£åˆ—ï¼ŒåŒ…å« `id` å’Œ `text`ã€‚

6. JSON è¼¸å‡ºç¯„ä¾‹
**è¼¸å…¥:**
[
    {"id": 0, "constraint": "é™ 15 éŸ³ç¯€", "content": "Sakuramotoæ®ºçƒ -> Tanæ“‹ç¶²"},
    {"id": 1, "constraint": "é™ 10 éŸ³ç¯€", "content": "ä¸­å ´é–“éš™ (Gap)"},
    {"id": 2, "constraint": "é™ 8 éŸ³ç¯€", "content": "æ®ºçƒ -> ç•«é¢ä¸­æ–·"}
]
**è¼¸å‡º:**
[
    {"id": 0, "text": "Sakuramotoèµ·è·³é‡æ®ºï¼Œä½†Tané˜²å®ˆå¾—éå¸¸ç©©å¥ï¼"},
    {"id": 1, "text": "é€™çƒé›™æ–¹ç¯€å¥éƒ½å¾ˆå¿«ï¼Œç¨å¾®å–˜å£æ°£ã€‚"},
    {"id": 2, "text": "é€™çƒæ®ºå¾—éå¸¸å…‡ï¼"}
]

ğŸ“Š **æœ¬æ®µå¾…è™•ç†åˆ—è¡¨ï¼š**
{{ event_data }}

è«‹è¼¸å‡º JSONï¼š
"""

prompt_builder = PromptBuilder(template=narrative_template, required_variables=["event_data","prev_context"])

add_video_s2 = AddVideo2Prompt()
gemini_s2 = GeminiGenerator(project_id="ai-anchor-462506", location="us-central1", model="gemini-2.5-flash")

pipeline_s2 = Pipeline()
pipeline_s2.add_component(instance=prompt_builder, name="prompt_builder")
pipeline_s2.add_component(instance=add_video_s2, name="add_video")
pipeline_s2.add_component(instance=gemini_s2, name="llm")
pipeline_s2.connect("prompt_builder.prompt", "add_video.prompt")
pipeline_s2.connect("add_video.prompt", "llm.prompt")

# ========== 6. è¼”åŠ©å‡½å¼ (å«æœ€çµ‚é˜²ç·š) ==========
def _flush_chunk(results_list, chunk_data, global_counter_ref):
    dur = chunk_data["end"] - chunk_data["start"]
    limit = int(dur * SYLLABLES_PER_SEC)
    should_keep = False
    
    # åˆ¤æ–·é¡å‹
    is_gap = "é–“éš™" in chunk_data.get("info", "") or "Gap" in chunk_data.get("info", "")
    is_crucial = chunk_data.get("is_crucial", False)

    # ğŸ”¥ æœ€çµ‚é˜²ç·šï¼šè‹¥é€šéäº†è¿´åœˆçš„ç¯©é¸ï¼Œé€™è£¡åšæœ€å¾Œçš„æ ¼å¼ä¿éšœ
    if is_crucial or is_gap:
        # é—œéµæ™‚åˆ»/é–“éš™ï¼šå¼·åˆ¶ä¿ç•™ä¸¦çµ¦äºˆæœ€å°å­—æ•¸ç©ºé–“
        limit = max(limit, 6) 
        should_keep = True
    else:
        # æ™®é€š Rallyï¼šè‹¥é‚„æ˜¯å¤ªçŸ­ä¸”æ²’è¢«åˆä½µï¼Œä¸Ÿæ£„ (é€™æ˜¯æœ€å¾Œä¸€é“æ¿¾ç¶²)
        if limit >= 4:
            should_keep = True

    if should_keep:
        results_list.append({
            "global_id": global_counter_ref[0], 
            "start_sec": chunk_data["start"], 
            "end_sec": chunk_data["end"], 
            "limit": limit, 
            "info": chunk_data["info"]
        })
        global_counter_ref[0] += 1


# ========== 7. æ ¸å¿ƒåŠŸèƒ½ï¼šè™•ç†å–®ä¸€å½±ç‰‡ ==========
def process_single_video_stage2(video_path, event_json_path, output_folder):
    global NARRATIVE_HISTORY

    os.makedirs(output_folder, exist_ok=True)
    base_name = os.path.splitext(os.path.basename(video_path))[0]
    
    try:
        with VideoFileClip(video_path) as clip: total_duration = clip.duration
    except: total_duration = 30.0 

    try:
        with open(event_json_path, 'r', encoding='utf-8') as f: 
            data = json.load(f)
            nested_events = data.get("events", []) if isinstance(data, dict) else data
            video_uri = data.get("segment_video_uri", "") if isinstance(data, dict) else ""
    except: return None

    if not nested_events: return None

    # --- A. æ•¸æ“šèšåˆé‚è¼¯ ---
    RALLY_TYPES = ["Exchange", "Attack", "Defend"] 
    chunk_events = [] 
    global_id_counter = [0] 
    last_committed_time = 0.0

    # 1. INTRO
    first_chunk_start = parse_time_str(nested_events[0].get("start_time", "0:00.0"))
    intro_limit = int(first_chunk_start * SYLLABLES_PER_SEC)
    if intro_limit >= 8 and intro_limit <= MAX_INTRO_OUTRO_SYLLABLES:
        chunk_events.append({
            "global_id": "INTRO",
            "start_sec": 0.0,
            "end_sec": first_chunk_start,
            "limit": intro_limit,
            "info": "é–‹å ´ç©ºç™½"
        })
        last_committed_time = first_chunk_start
    else:
        last_committed_time = 0.0
    
    # 2. èšåˆè¿´åœˆ (å«å‘å¾Œåˆä½µé‚è¼¯)
    buffer_chunk = None

    for chunk in nested_events:
        chunk_start = parse_time_str(chunk.get("start_time", "0:00.0"))
        chunk_end = parse_time_str(chunk.get("end_time", "0:00.0"))
        inner_list = chunk.get("events", [])
        
        if not inner_list: continue

        actions_str = " -> ".join([f"{ev.get('player')}{ev.get('action')}" for ev in inner_list])
        is_crucial = any(ev.get('is_crucial') is True for ev in inner_list)
        is_pure_rally = all(ev.get('category') in RALLY_TYPES for ev in inner_list) and not is_crucial

        current_chunk = {
            "start": chunk_start, 
            "end": chunk_end, 
            "info": actions_str,
            "is_rally": is_pure_rally,
            "is_crucial": is_crucial 
        }

        # --- Gap Detection ---
        prev_end_candidate = buffer_chunk["end"] if buffer_chunk else last_committed_time
        gap_duration = chunk_start - prev_end_candidate
        
        if gap_duration > MIN_GAP_DURATION:
            # ç™¼ç¾å¤§é–“éš™
            if buffer_chunk:
                # æª¢æŸ¥ buffer æ˜¯å¦å¤ªçŸ­ï¼Ÿè‹¥æ˜¯ï¼Œç›´æ¥è¢« Gap åå™¬ (åˆªé™¤ buffer)
                # é€™é¿å…ç”¢ç”Ÿ "Action (0.2s) -> Gap" çš„æ€ªç•°çµæ§‹
                buf_dur = buffer_chunk["end"] - buffer_chunk["start"]
                if buf_dur < MERGE_THRESHOLD:
                    # åå™¬ï¼šGap èµ·é»æå‰åˆ° buffer èµ·é»
                    prev_end_candidate = buffer_chunk["start"]
                    buffer_chunk = None # ä¸Ÿæ£„ buffer
                else:
                    # æ­£å¸¸çµç®—
                    _flush_chunk(chunk_events, buffer_chunk, global_id_counter)
                    last_committed_time = buffer_chunk["end"]
                    buffer_chunk = None
                    prev_end_candidate = last_committed_time
            
            # æ’å…¥é–“éš™äº‹ä»¶
            gap_chunk = {
                "start": prev_end_candidate,
                "end": chunk_start,
                "info": "ä¸­å ´é–“éš™ (Gap)",
                "is_crucial": False
            }
            _flush_chunk(chunk_events, gap_chunk, global_id_counter)
            last_committed_time = chunk_start
            buffer_chunk = current_chunk

        else:
            # --- æ­£å¸¸åˆä½µé‚è¼¯ ---
            if buffer_chunk:
                potential_dur = current_chunk["end"] - buffer_chunk["start"]
                is_mergeable = (
                    buffer_chunk["is_rally"] and 
                    current_chunk["is_rally"] and 
                    potential_dur <= MAX_RALLY_DURATION
                )
                
                if is_mergeable:
                    # æ¨™æº–åˆä½µï¼šå‘å¾Œå»¶ä¼¸
                    buffer_chunk["end"] = current_chunk["end"] 
                    buffer_chunk["info"] += f" -> {current_chunk['info']}"
                else:
                    # è¡çªï¼šç„¡æ³•æ¨™æº–åˆä½µ
                    # ğŸ”¥ [æ–°å¢] å¼·åˆ¶å‘å¾Œåˆä½µæª¢æŸ¥ (Force Merge Forward)
                    # å¦‚æœ buffer å¯¦åœ¨å¤ªçŸ­ (ä¾‹å¦‚ 0.2s)ï¼Œç‚ºäº†ä¸æµªè²»ï¼Œå¼·åˆ¶å¡çµ¦ current
                    buf_dur = buffer_chunk["end"] - buffer_chunk["start"]
                    
                    if buf_dur < MERGE_THRESHOLD:
                        # åŸ·è¡Œå‘å¾Œåˆä½µï¼šCurrent å¸æ”¶ Buffer
                        current_chunk["start"] = buffer_chunk["start"] # æ™‚é–“å‰æ¨
                        current_chunk["info"] = f"{buffer_chunk['info']} -> {current_chunk['info']}" # å…§å®¹å‰ç½®
                        
                        # å±¬æ€§ç¹¼æ‰¿ï¼šè‹¥ buffer æ˜¯é—œéµï¼Œåˆä½µå¾Œä¹Ÿè¦–ç‚ºé—œéµ (é¿å…æ¼å ±)
                        if buffer_chunk["is_crucial"]:
                            current_chunk["is_crucial"] = True
                        
                        # Buffer è¢«å¸æ”¶ï¼Œç¾åœ¨ Current è®Šæˆæ–°çš„ Buffer
                        buffer_chunk = current_chunk
                    else:
                        # Buffer å¤ é•·ï¼Œå¯ä»¥ç¨ç«‹ç”Ÿå­˜
                        _flush_chunk(chunk_events, buffer_chunk, global_id_counter)
                        last_committed_time = buffer_chunk["end"]
                        buffer_chunk = current_chunk
            else:
                buffer_chunk = current_chunk
        
    # çµç®—æœ€å¾Œçš„ buffer
    if buffer_chunk:
        # æœ€å¾Œä¸€æ®µç„¡æ³•å‘å¾Œåˆä½µï¼Œåªèƒ½ä¾é  _flush_chunk çš„ padding ä¿è­·
        _flush_chunk(chunk_events, buffer_chunk, global_id_counter)
        last_committed_time = buffer_chunk["end"]

    # 3. OUTRO
    outro_dur = total_duration - last_committed_time
    outro_limit = int(outro_dur * SYLLABLES_PER_SEC)
    
    if outro_limit >= 8 and outro_limit <= MAX_INTRO_OUTRO_SYLLABLES:
        chunk_events.append({
            "global_id": "OUTRO", 
            "start_sec": last_committed_time, 
            "end_sec": total_duration, 
            "limit": outro_limit, 
            "info": "çµå°¾ç©ºç™½"
        })

    # --- B. å‘¼å« LLM ---
    if NARRATIVE_HISTORY:
        recent_history = NARRATIVE_HISTORY[-HISTORY_WINDOW_SIZE:]
        history_str = "\n".join([f"- {h}" for h in recent_history])
    else:
        history_str = "é€™æ˜¯æ¯”è³½çš„ç¬¬ä¸€å€‹ç‰‡æ®µï¼Œè«‹é–‹å§‹ç²¾å½©çš„è§£èªªã€‚"

    llm_input_data = []
    for e in chunk_events:
        llm_input_data.append({"id": e["global_id"], "constraint": f"é™ {e['limit']} éŸ³ç¯€", "content": e["info"]})
    
    try:
        res = pipeline_s2.run({
            "add_video": {"uri": video_uri},
            "prompt_builder": {
                "event_data": json.dumps(llm_input_data, ensure_ascii=False, indent=2),
                "prev_context": history_str 
                }
        })
        reply = res["llm"]["replies"][0].strip()
        if reply.startswith("```"): reply = reply.split("\n", 1)[1].rsplit("\n", 1)[0]
        generated_list = json.loads(reply)
        generated_map = {str(item["id"]): item["text"] for item in generated_list}
    except Exception as e:
        print(f"âŒ [Stage 2 éŒ¯èª¤] {e}")
        return None

    # --- C. è¼¸å‡ºçµæœ ---
    commentary = []
    current_segment_narrative = [] 
    
    for chunk in chunk_events:
        gid = str(chunk["global_id"])
        text_content = generated_map.get(gid)
        if not text_content: continue 

        duration = chunk["end_sec"] - chunk["start_sec"]
        
        validation_duration = duration
        if gid in ["INTRO", "OUTRO"]: validation_duration = min(duration, 5.0)
        
        estimated_dur = estimate_speech_time(text_content)
        if estimated_dur > (validation_duration * 1.2):
            ratio = (validation_duration * 1.2) / estimated_dur
            safe_length = int(len(text_content) * ratio)
            text_content = text_content[:safe_length].rstrip("ï¼Œ,")

        chunk_info_lower = chunk["info"].lower()
        if "gap" in chunk_info_lower:
            emotion = "å¹³ç©©"
        else:
            emotion = "æ¿€å‹•" if "æ®ºçƒ" in chunk_info_lower or "å¾—åˆ†" in chunk_info_lower or "attack" in chunk_info_lower else "å¹³ç©©"

        if commentary and len(text_content) >= 2 and len(commentary[-1]["text"]) >= 2:
            check_len = min(5, len(text_content), len(commentary[-1]["text"]))
            if text_content[:check_len] == commentary[-1]["text"][:check_len]:
                commentary[-1]["end_time"] = seconds_to_timecode(chunk["end_sec"])
                prev_start = parse_time_str(commentary[-1]["start_time"])
                new_dur = chunk["end_sec"] - prev_start
                commentary[-1]["time_range"] = format_duration(new_dur)
                continue
        
        if text_content:
            current_segment_narrative.append(text_content)

        commentary.append({
            "start_time": seconds_to_timecode(chunk["start_sec"]),
            "end_time": seconds_to_timecode(chunk["end_sec"]),
            "time_range": format_duration(duration),
            "emotion": emotion,
            "text": text_content
        })

    if current_segment_narrative:
        full_segment_text = " ".join(current_segment_narrative)
        NARRATIVE_HISTORY.append(full_segment_text)
        if len(NARRATIVE_HISTORY) > 20: 
            NARRATIVE_HISTORY.pop(0)

    output_filename = f"{base_name}.json"
    output_path = os.path.join(output_folder, output_filename)
    if commentary:
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump({"segment": os.path.basename(video_path), "commentary": commentary}, f, ensure_ascii=False, indent=2)
        return output_path
    else:
        return None

# ========== 8. ç¨ç«‹é‹è¡Œæ¨¡å¼ ==========
if __name__ == "__main__":
    video_folder = "D:/Vs.code/AI_Anchor/backend/video_splitter/badminton_segments"
    event_json_folder = "D:/Vs.code/AI_Anchor/backend/gemini/event_analysis_output"
    output_folder = "D:/Vs.code/AI_Anchor/backend/gemini/final_narratives"
    
    NARRATIVE_HISTORY = [] 
    
    print(f"\nğŸš€ [ç¨ç«‹æ¨¡å¼] Stage 2 (å‘å¾Œåˆä½µå¢å¼·ç‰ˆ) æ‰¹æ¬¡å•Ÿå‹•...")
    if os.path.exists(event_json_folder):
        files = sorted([f for f in os.listdir(event_json_folder) if f.endswith("_event.json")])
        for f in tqdm(files, desc="Processing"):
            base = f.replace("_event.json", "")
            vid_path = os.path.join(video_folder, f"{base}.mp4")
            json_path = os.path.join(event_json_folder, f)
            if os.path.exists(vid_path):
                res = process_single_video_stage2(vid_path, json_path, output_folder)
                if res: print(f"  -> Saved: {os.path.basename(res)}")
    else:
        print("âŒ æ‰¾ä¸åˆ° JSON è³‡æ–™å¤¾")